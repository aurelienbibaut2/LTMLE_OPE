behavior_action_matrix,
transition_based_rewards,
horizon)
pi_b <- apply(H, 1, function(x) behavior_action_matrix[x[1], x[2]]) # propensity score
pi_a <- apply(H, 1, function(x) evaluation_action_matrix[x[1], x[2]]) # probabilities of actions taken in H under the evaluation policy
pi_b
pi_a
D <- aperm(replicate(5, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
D
dim(D)
D <- aperm(replicate(3, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
dim(D)
D[1,,]
D[,,'pi_a']
D[,,'pi_a'] / D[,,'pi_b']
apply(D[,, 'pi_a'] / D[,, 'pi_b'], 2, cumprod)
apply(D[,, 'pi_a'] / D[,, 'pi_b'], 1, cumprod)
V0
Q0
Q0
dim(Q0)
D[ , 20, ]
D[ , 20, 's']
D[ , 20, 'a']
t=20
D[ , t, ]
D[ , t, ][, 'a']
Q0[20, , ]
Q_t <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']])
Q_t
n
D <- aperm(replicate(7, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
V_tilde <- rep(0, n)
Delta_t <- horizon + 1 - t
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde
Q_t <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']])
n <- 7
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
V_tilde <- rep(0, n)
Delta_t <- horizon + 1 - t
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde
Q_t <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']])
Q_t
D
dim(D)
generate_discrete_MDP_trajectory_with_pi_a_pi_b <- function(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon){
H <- generate_discrete_MDP_trajectory(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)
pi_b <- apply(H, 1, function(x) behavior_action_matrix[x[1], x[2]]) # propensity score
pi_a <- apply(H, 1, function(x) evaluation_action_matrix[x[1], x[2]]) # probabilities of actions taken in H under the evaluation policy
rho_t <- cumprod(pi_a/pi_b)
cbind(H, pi_a=pi_a, pi_b=pi_b, rho_t=rho_t)
}
n <- 7
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
D[,20,]
Delta_t <- horizon + 1 - t
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde
Q_t <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']])
Q_tilde_t <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t)) + 1,
family=binomial, weights = D[,t, 'rho_t'])
# Expit and logit
expit <- function(x) { 1 / (1 + exp(-x)) }
logit <- function(x) { log(x / (1 - x)) }
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t)) + 1,
family=binomial, weights = D[,t, 'rho_t'])
epsilon
# LTMLE -------------------------------------------------------------------
# Generate dataset
n <- 7
generate_discrete_MDP_trajectory_with_pi_a_pi_b <- function(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon){
H <- generate_discrete_MDP_trajectory(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)
pi_b <- apply(H, 1, function(x) behavior_action_matrix[x[1], x[2]]) # propensity score
pi_a <- apply(H, 1, function(x) evaluation_action_matrix[x[1], x[2]]) # probabilities of actions taken in H under the evaluation policy
rho_t <- cumprod(pi_a/pi_b)
cbind(H, pi_a=pi_a, pi_b=pi_b, rho_t=rho_t)
}
n <- 7
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
V_tilde <- rep(0, n)
t = 20
Delta_t <- horizon + 1 - t
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde
Q_t <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']])
Q_tilde_t <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t)) + 1,
family=binomial, weights = D[,t, 'rho_t'])
R_tilde
Q_tilde
Q_tilde_t
epsilon
epsilon$coefficients[1]
Q0[t]
Q0[t,,]
dim(Q0)
D[,t, 'rho_t']
D[,,'rho_t']
t = 20
V <- rep(0, n)
Delta_t <- horizon + 1 - t
V_tilde <- (V + Delta_t) / (2 * Delta_t)
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=binomial, weights = D[,t, 'rho_t'])$coefficients[1]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit(Q0[t, ,] + epsilon))
Q0[t, ,]
x=0
log(x/(1-x))
x=1
log(x/(1-x))
logit(0)
logit(1)
expit(logit(0) + 0)
expit(logit(1) + 0)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit(Q0[t, ,] + epsilon))
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit(Q0[t, ,]) + epsilon)
logit(Q0[t, ,])
Q0[t,,]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon)
t=20
V <- rep(0, n)
Delta_t <- horizon + 1 - t
V_tilde <- (V + Delta_t) / (2 * Delta_t)
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
U_tilde <- R_tilde + V_tilde # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=binomial, weights = D[,t, 'rho_t'])$coefficients[1]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon)
# Expit and logit
expit <- function(x) { 1 / (1 + exp(-x)) }
logit <- function(x) { log(x / (1 - x)) }
Q_tilde_t
epsilon
V <- rep(0, n)
Delta_t <- horizon + 1 - t
V_tilde <- (V + Delta_t) / (2 * Delta_t)
V_tilde
R_tilde <- (D[, t, 'r'] + Delta_t) / (2 * Delta_t)
R_tilde
U_tilde <- R_tilde + V_tilde # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
U_tilde
V_tilde
V <- rep(0, n)
t
V <- rep(0, n) #V_{t+1}(S_{t+1})
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=binomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilon
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon)
Q_tilde_t
evaluation_action_matrix
as.tensor(Q_tilde_t, dims=c(s=2, a=2))
as.tensor(Q_tilde_t, dims=c(s=3, a=2))
as.tensor(evaluation_action_matrix, dims=c(s=3,a=2))
Q_tilde_t * evaluation_action_matrix
0.27 * 0.4
0.73 * 0.6
apply(Q_tilde_t * evaluation_action_matrix, 1, sum)
0.108+0.438
n <- 7
generate_discrete_MDP_trajectory_with_pi_a_pi_b <- function(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon){
H <- generate_discrete_MDP_trajectory(s0, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)
pi_b <- apply(H, 1, function(x) behavior_action_matrix[x[1], x[2]]) # propensity score
pi_a <- apply(H, 1, function(x) evaluation_action_matrix[x[1], x[2]]) # probabilities of actions taken in H under the evaluation policy
rho_t <- cumprod(pi_a/pi_b)
cbind(H, pi_a=pi_a, pi_b=pi_b, rho_t=rho_t)
}
n <- 7
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
V <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=binomial, weights = D[,t, 'rho_t'])$coefficients[1]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# V is gonna be V_{t+1} in the next iteration
}
epsilon
warnings()
V <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# V is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
warnings()
V
R
V
D[, t, 's']
V[D[, t, 's']]
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
epsilon
# LTMLE on an example
n <- 7
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
print(epsilons)
# LTMLE on an example
n <- 100
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
print(epsilons)
# LTMLE on an example
n <- 100
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
V_evaluated
t
Delta_t <- horizon + 1 - t
Delta_t
R <- D[, t, 'r'] # R_t
R
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
U_tilde
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t_evaluated + Delta_t) / (2 * Delta_t)
Q_t_evaluated
Q_tilde_t_evaluated
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilon
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
Q_tilde_t_star
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
V_tilde
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
V
0.73 * 0.2 + 0.23 * -0.2
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
V_evaluated
t = 19
Delta_t <- horizon + 1 - t
# LTMLE on an example
n <- 100
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t_evaluated + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
print(epsilons)
# LTMLE on an example
n <- 1000
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t_evaluated + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
print(epsilons)
# LTMLE on an example
n <- 10000
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t_evaluated + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
print(epsilons)
# LTMLE on an example
n <- 10000
D <- aperm(replicate(n, generate_discrete_MDP_trajectory_with_pi_a_pi_b(1, state_transition_matrix,
behavior_action_matrix,
transition_based_rewards,
horizon)), c(3,1,2))
epsilons <- c()
V_evaluated <- rep(0, n) #V_{t+1}(S_{t+1})
for(t in horizon:1){
Delta_t <- horizon + 1 - t
R <- D[, t, 'r'] # R_t
U_tilde <- (R + V_evaluated + Delta_t) / (2 * Delta_t) # U_tilde = R_tilde_t + V_tilde_{t+1}(S_t) in the notations of the write-up
Q_t_evaluated <- apply(D[ , t, ], 1, function(x) Q0[t, x['s'], x['a']]) # Q_t(A_t, S_t)
Q_tilde_t_evaluated <- (Q_t_evaluated + Delta_t) / (2 * Delta_t)
epsilon <- glm(U_tilde ~ offset(logit(Q_tilde_t_evaluated)) + 1,
family=quasibinomial, weights = D[,t, 'rho_t'])$coefficients[1]
epsilons <- c(epsilons, epsilon)
# Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2
Q_tilde_t_star <- expit(logit((Q0[t, ,] + Delta_t) / (2 * Delta_t)) + epsilon) # Q_tilde_t^*
# Then set V_tilde(s_t) = \sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)
V_tilde <- apply(Q_tilde_t_star * evaluation_action_matrix, 1, sum)
# Compute V = 2 * Delta_t * (V_tilde - 1)
V <- 2 * Delta_t * (V_tilde - 1/2)
# Evaluate V
V_evaluated <-  V[D[, t, 's']]
# V_tilde is gonna be V_{t+1} in the next iteration
}
# The average of the last V is the LTML estimator of the value
V_hat_LTMLE <- mean(V_evaluated)
#print(epsilons)
cat('V_hat_LTMLE: ', V_hat_LTMLE, '\n')
cat('V0: ', V0[1,1], '\n')
source('~/aurelien.bibaut@gmail.com/Data_PC/PhD_Berkeley/LTMLE_OPE/R/Experiments.R')
source('~/aurelien.bibaut@gmail.com/Data_PC/PhD_Berkeley/LTMLE_OPE/R/Experiments.R')
library(dplyr)
