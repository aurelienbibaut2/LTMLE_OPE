{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelWin_env as modelWin\n",
    "import discrete_MDP_transition_based_rewards as MDP\n",
    "import generate_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPS(D):\n",
    "    horizon = D.shape[1]\n",
    "    return np.mean(D[:, horizon-1, 5] * np.sum(D[:, :, 2], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepIPS(D):\n",
    "    return np.mean(np.sum(D[:, :, 5] * D[:, :, 2], axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WIPS(D):\n",
    "    horizon = D.shape[1]\n",
    "    w_H = np.mean(D[:, horizon-1, 5])\n",
    "    return np.mean(D[:, horizon-1, 5] / w_H * np.sum(D[:, :, 2], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepWIPS(D):\n",
    "    n = D.shape[0]\n",
    "    horizon = D.shape[1]\n",
    "    w_t = np.mean(D[:, :, 5], axis=0)\n",
    "    return np.mean(np.sum(D[:, :, 5] / (np.ones((n,1)) * w_t.T) * D[:, :, 2], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DR(D, Q_hat, V_hat):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    n = D.shape[0]\n",
    "    horizon = D.shape[1]\n",
    "    D_star = np.zeros((n, horizon))\n",
    "\n",
    "    t = horizon-1\n",
    "    D_star[:, t] = D[:, t, 5] * (D[:, t, 2] - Q_hat[t, D[:, t, 0].astype(int), D[:, t, 1].astype(int)])\n",
    "    for t in np.arange(0, horizon-1)[::-1]:\n",
    "        D_star[:, t] = D[:, t, 5] * (D[:, t, 2] + V_hat[t+1, D[:, t+1, 0].astype(int)]\n",
    "                                      - Q_hat[t, D[:, t, 0].astype(int), D[:, t, 1].astype(int)])\n",
    "    return np.mean(V_hat[t, D[:, t, 0].astype(int)] + np.sum(D_star, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WDR(D, Q_hat, V_hat):\n",
    "    n = D.shape[0]\n",
    "    horizon = D.shape[1]\n",
    "    D_star = np.zeros((n, horizon))\n",
    "    w_t = np.mean(D[:, :, 5], axis=0)\n",
    "    \n",
    "    t = horizon-1\n",
    "    D_star[:, t] = D[:, t, 5] / w_t[t] * (D[:, t, 2] - Q_hat[t, D[:, t, 0].astype(int), D[:, t, 1].astype(int)])\n",
    "    for t in np.arange(0, horizon-1)[::-1]:\n",
    "        D_star[:, t] = D[:, t, 5] / w_t[t] * (D[:, t, 2] + V_hat[t+1, D[:, t+1, 0].astype(int)]\n",
    "                                      - Q_hat[t, D[:, t, 0].astype(int), D[:, t, 1].astype(int)])\n",
    "    return np.mean(V_hat[t, D[:, t, 0].astype(int)] + np.sum(D_star, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return np.log(x) - np.log(1 - x)\n",
    "def expit(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epsilon(y, Q, weights, epsilon0, max_it):\n",
    "    \"\"\"Fit epsilon in the targeting step, using IRLS.\n",
    "    The argument Q here is the \\tilde{Q}(A^(i)_t, S^(i)_t)\n",
    "    from the write-up\"\"\"\n",
    "    epsilon = epsilon0\n",
    "    for it in range(max_it):\n",
    "        ll = -np.sum(weights * (y * np.log(expit(logit(Q) + epsilon))\n",
    "                                +(1-y) * np.log(1 - expit(logit(Q) + epsilon)) ) )\n",
    "        grad = -np.sum(weights * (y - expit(logit(Q) + epsilon)) )\n",
    "        hessian = np.sum( weights * expit(logit(Q) + epsilon)\n",
    "                         * (1 - expit(logit(Q) + epsilon)) )\n",
    "        epsilon = epsilon - grad / hessian\n",
    "        \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LTMLE(D, Q_hat, V_hat, evaluation_policy_matrix):\n",
    "    horizon = D.shape[1]; n = D.shape[0]\n",
    "\n",
    "    V_evaluated = np.zeros(n)\n",
    "\n",
    "    epsilons = []\n",
    "    for t in np.arange(0, horizon)[::-1]:\n",
    "        Delta_t = horizon - t\n",
    "        R = D[:, t, 2]\n",
    "        U_tilde = (R + V_evaluated + Delta_t) / (2 * Delta_t)\n",
    "        Q_t_evaluated = Q_hat[t, D[:, t, 0].astype(int), D[:, t, 1].astype(int)]\n",
    "        Q_tilde_evaluated = (Q_t_evaluated + Delta_t) / (2 * Delta_t)\n",
    "        # Targeting step\n",
    "        epsilons.append(fit_epsilon(y=U_tilde, Q=Q_tilde_evaluated, weights=D[:, t, 5],\n",
    "                                   epsilon0=0, max_it=5))\n",
    "        # Evaluate Q_tilde(s_t, a) for a_t = 1, a_t = 2\n",
    "        Q_tilde_t_star = expit(logit((Q_hat[t, :, :] + Delta_t) / (2 * Delta_t)) + epsilons[-1])\n",
    "        # Then set V_tilde(s_t) = \\sum_{a} Q_tilde(s_t, a) pi_a(a|s_t)\n",
    "        V_tilde = np.einsum('sa,sa->s', Q_tilde_t_star, evaluation_policy_matrix)\n",
    "        # Compute V = 2 * Delta_t * (V_tilde - 1)\n",
    "        V = 2 * Delta_t * (V_tilde - 1/2)\n",
    "        # Evaluate V\n",
    "        V_evaluated = V[D[:, t, 0].astype(int)]\n",
    "    return np.mean(V_evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.28125444079999995\n",
      "-0.4380213758338713\n",
      "0.02188125109999997\n",
      "-0.04880679305197754\n",
      "0.1354181801599999\n",
      "0.03222255559277132\n"
     ]
    }
   ],
   "source": [
    "# Compare results with the R implementation\n",
    "n = 100; horizon = 5; n_columns = 8; n_states = 3; n_actions = 2\n",
    "D = np.genfromtxt('R/data.csv', delimiter=',').reshape((n, horizon, n_columns), order='F')\n",
    "# Need to subtract 1 from the states and actions indices\n",
    "D[:, :, 0] -= 1\n",
    "D[:, :, 1] -= 1\n",
    "Q_hat = np.genfromtxt('R/Q_hat.csv', delimiter=',').reshape((horizon, n_states, n_actions), order='F')\n",
    "V_hat = np.genfromtxt('R/V_hat.csv', delimiter=',')\n",
    "print(IPS(D))\n",
    "print(WIPS(D))\n",
    "print(stepIPS(D))\n",
    "print(stepWIPS(D))\n",
    "print(DR(D, Q_hat, V_hat))\n",
    "print(LTMLE(D, Q_hat, V_hat, modelWin.evaluation_policy_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try the estimators on an environment\n",
    "# First, instantiate an environmenent\n",
    "s0 = 0\n",
    "env = MDP.discrete_MDP_transition_based_rewards(modelWin.state_transition_matrix,\n",
    "                                                modelWin.transition_based_rewards)\n",
    "env.reset(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some data\n",
    "n = 10000; horizon = 5\n",
    "D = generate_dataset.generate_dataset(env, s0, horizon, n,\n",
    "                                      modelWin.behavior_action_matrix,\n",
    "                                      modelWin.evaluation_policy_matrix)\n",
    "D.shape\n",
    "# columns of D: s, a, r, pi_b, pi_e, rho_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.276, 0.184, 0.184])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the true value of the policy value\n",
    "Q0, V0 = env.get_true_Q_and_V_functions(modelWin.evaluation_policy_matrix, 1, horizon)\n",
    "V0[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.288666876295068\n",
      "0.30934108673038707\n",
      "0.29705044158721056\n",
      "0.3161740955037919\n",
      "0.32034237330683163\n",
      "0.3211496515689108\n"
     ]
    }
   ],
   "source": [
    "print(IPS(D))\n",
    "print(stepIPS(D))\n",
    "print(WIPS(D))\n",
    "print(stepWIPS(D))\n",
    "print(DR(D, Q0, V0))\n",
    "print(WDR(D, Q0, V0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
